from gensim.models import Word2Vec
import pandas as pd
import numpy as np
import networkx as nx
from sklearn.cluster import KMeans
from based import *
from functools import reduce
from walker import RandomWalker

class Embedding:

    def __init__(self, graphs, workers=1,verbose=0):

        self.graphs = graphs
        self._embeddings = {}
        self.nodes = list(reduce(lambda x, y: x | y, [set(g.nodes()) for g in self.graphs]))
        self.walker = RandomWalker(graphs,nodes=self.nodes,verbose=verbose)
        self.workers=workers
        self.verbose=verbose
        #print("Preprocess transition probs...")
        for g in graphs:
            self.walker.preprocess_transition_probs(g)

    def train(self, embed_size=128, window_size=5,epoch=5, **kwargs):

        kwargs["sentences"] = self.sentences
        kwargs["min_count"] = kwargs.get("min_count", 0)
        kwargs["size"] = embed_size
        kwargs["sg"] = 1  #select sg mode
        kwargs["hs"] = 0  #not use Hierarchical Softmax
        kwargs["workers"] = self.workers
        kwargs["window"] = window_size
        kwargs["iter"] = epoch

        print("Learning embedding vectors...")
        model = Word2Vec(**kwargs)
        print("Learning embedding vectors done!")

        self.w2v_model = model

        return model

    def sample_sentence(self,num_walks,walk_length):
        #print("sampling sentences...")
        self.sentences = self.walker.simulate_walks(
            num_walks=num_walks, walk_length=walk_length, workers=self.workers)

    def get_embeddings(self,samples=None):
        if self.w2v_model is None:
            print("model not train")
            return np.array([])

        self._embeddings = {}
        if(samples is None):
            samples=self.nodes
        for word in samples:
            self._embeddings[word] = self.w2v_model.wv[word]

        embeddings=self._embeddings
        emb_list = []
        for k in embeddings:
            emb_list.append(embeddings[k])
        emb_list = np.array(emb_list)
        emb_list = emb_list / np.sqrt((emb_list * emb_list).sum(axis=1).reshape(-1, 1))
        return pd.DataFrame(emb_list,index=list(samples))

def MSNE(views, n_clusters=5, k=20,workers=4, walk_length=20, num_walks=100, embed_size=100, window_size=10):

    """
    MSNE is a multi-omics integrative clustering method for cancer subtyping, especially when the
    multi-omics dataset is partial (e.g. some samples have only a subset of omics data). MSNE construct
    similarity network for each omics data, and then embedding the multiple similarity networks to
    d-dimensional vector space. Kmeans is used to cluster the samples finally.

    :param views: the list of pandas.DataFrame(i.e. omics data). each row in omics data is a sample, each column in omics
     data is a feature. the index of omics data will be considered as the name of sample.

    :param n_clusters: int, default 10. The number of clusters for Kmeans.

    :param k: int, default 20. The top k neighborhoods of each node will be treated as local neighbors.

    :param workers: int, default 4. The number of parallel threads.

    :param walk_length: int,default 20. The length of sequences generated by random walk on multiple networks.

    :param num_walks: int, default 100. Starting with each node, MSNE will generate 'num_walks' sequences.

    :param embed_size: int, default 100. the dimension of embedding vectors.

    :param window_size: int, default 10. the window_size in skip-gram.

    :return: The dict with elements:
        embeddings: pandas.DataFrame, the low dimensional vector representation of each samples.
        group: pandas.DataFrame, the clustering of samples.

    #########################
    example:
    import pandas as pd
    from embedding import MSNE
    from sklearn.manifold import TSNE
    import matplotlib.pyplot as plt
    from sklearn.metrics import normalized_mutual_info_score as nmi

    # read the Pixel and Fourier views of handwritten datasets.
    view1=pd.read_csv("../handwritten/mfeat-fou.csv",index_col=0)
    view2=pd.read_csv("../handwritten/mfeat-pix.csv",index_col=0)

    #apply MSNE on the multi-view dataset.
    result=MSNE([view1,view2],
                n_clusters=10,k=20,workers=4,
                walk_length=20,num_walks=20,
                embed_size=100,window_size=10)

    #sort the samples by name
    embeddings=result["embeddings"].reindex(samples)
    group=result["group"].reindex(samples).values.reshape(-1)

    #show the result of MSNE
    low=TSNE(n_components=2).fit_transform(embeddings)
    plt.scatter(low[:,0],low[:,1],c=group)
    print("NMI is:",nmi(label,group,average_method='geometric'))


    #########################
    Author: Han Xu, 413180435@qq.com
    #########################
    """

    S = []
    for df in views:
        s=similarity(df,k=k)
        S.append(nx.DiGraph(s))
    print("construct similarity networks finished")
    model = Embedding(S, workers=workers, verbose=0)
    model.sample_sentence(walk_length=walk_length, num_walks=num_walks)
    print("sample sequences finished")
    model.train(embed_size=embed_size, window_size=window_size, epoch=5)
    embeddings = model.get_embeddings()
    pre = KMeans(n_clusters=n_clusters, n_init=100).fit_predict(embeddings)
    print("get embedding vector and clusters")
    pre=pd.DataFrame(pre, index=embeddings.index)
    return dict(embeddings=embeddings,group=pre)